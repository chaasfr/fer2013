{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinaison de classifieurs \n",
    "===========================\n",
    "\n",
    "Il est toujours utile de combiner de multiples classifieurs pour améliorer les performances des systèmes de classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types de combinaisons\n",
    "--------------------\n",
    "\n",
    "Il existe plusieurs façons de combiner des classifieurs.\n",
    "- On peut faire voter chaque classifieur. Et classer un exemple par vote majoritaire, dans la classe qui a recueilli le maximum de votes. On peut pondérer ces votes en fonction de la confiance que l'on a dans la performance du classifieur. En général on préfère utiliser au moins 3 classifieurs.\n",
    "- On peut combiner les scores des différents classifieurs de façon plus complexe. On calcule un score global par classe à partir des scores de cette classe obtenus par chacun des classiieurs. On en calcule la moyenne arithmétique ou géométrique, le maximum, la variance. Et on classe en fonction de ce score global.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quels modèles combiner ?\n",
    "----------------------\n",
    "\n",
    "Il est d'usage de considérer que plus les classifieurs que l'on combine sont différents meilleure est leur combinaison. Il existe plusieurs façons d'obtenir des classifieurs divers.\n",
    "- On peut faire varier les données d'apprentissage. SI on dispose de N exemples d'apprentissage on crée K ensembles d'apprentissage de taille N en tirant avec remise dans l'ensemble original de train un ensemble d'apprentissage de N exemples. Et on apprend un modèle à partir de chaque ensemble d'apprentissage.\n",
    "- On utilise des modèles dont on fait varier certains paramètres. Par exemple on peut apprendre des Réseaux de Neurones avec différents nombred de couches cachées, des filtres de convolution différents, des réseaux avec et sans convolutions etc. On peut aussi utiliser la mpeme architecture et compter sur l'initialisation aléatoire et la convergence vers un minimum local pour assurer une certaine diversité.\n",
    "- On utilise différents types de classifieurs, svm, RN, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre travail\n",
    "------------\n",
    "Le code qui vous est donné ci-dessous permet de récupérer un réseau de neurones appris et de l'utiliser en TEST (sans réapprentissage) pour obtenir ses prédictions sur les données de test. Vous devrez utiliser ce code pour concevoir un système de classification plus performant sur les données d'émotions.\n",
    "Vous avez donc à :\n",
    "- Définir les modèles que vous souhaitez apprendre sur les données de train. EN cherchant à privilégier une certaine diversité dans les modèles. Vous devez enseuite apprendre ces modèles avec caffe.\n",
    "- Utiliser le code ci-dessous pour récupérer les prédictions (ou en modifiant un peu le code, les scores par classe et par exemple) de vos modèles.\n",
    "- Définir la façon de combiner les scores et écrire le code python correspondant. Puis évaluer en test la performance obtenue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des prédictions d'un réseau Caffe \n",
    "====================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2\n",
      "['/home/christian/caffe/build/python', '/home/christian/caffe/build/python', '', '/home/christian/caffe/python', '/home/christian/caffe/build/python', '/home/christian/deep/rendu', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/christian/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/usr/lib/pymodules/python2.7', '/usr/lib/python2.7/dist-packages/ubuntu-sso-client', '/usr/local/lib/python2.7/dist-packages/IPython/extensions', '/home/christian/.ipython']\n",
      "/home/christian/caffe/build/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print np.__version__\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "caffe_root=\"/home/christian/caffe/build/\"\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "TP_root=\"/home/christian/deep/rendu/\"\n",
    "\n",
    "print sys.path \n",
    "print caffe_root\n",
    "\n",
    "import caffe\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "\n",
    "nb_donnees=3500 # Nb données de test\n",
    "outfile_labels = TP_root+\"labels\"\n",
    "outfile_public_labels = TP_root+\"public_labels\"\n",
    "\n",
    "#caffe.set_mode_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520571428571\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET et génère ses prédictions sur le private test\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_train_test.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_train_test_iter_10000.caffemodel\"\n",
    "outfile_lenet = TP_root+\"predictions/predictions_lenet\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_lenet+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_lenet+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_lenet,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.479714285714\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_monoconv et génère ses prédictions sur le private test\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_monoconv.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_monoconv_iter_10000.caffemodel\"\n",
    "outfile_monoconv = TP_root+\"predictions/predictions_monoconv\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_monoconv+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_monoconv+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_monoconv,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470285714286\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_tripleconv et génère ses prédictions sur le private test\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_tripleconv.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_tripleconv_iter_10000.caffemodel\"\n",
    "outfile_tripleconv = TP_root+\"predictions/predictions_tripleconv\"\n",
    "derniere_couche=\"ip1\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_tripleconv+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_tripleconv+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_tripleconv,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454857142857\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_3conv2ip et génère ses prédictions sur le private test\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_3conv2ip.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_3conv2ip_iter_10000.caffemodel\"\n",
    "outfile_3conv2ip = TP_root+\"predictions/predictions_3conv2ip\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_3conv2ip+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_3conv2ip+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_3conv2ip,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.468\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_microsoft et génère ses prédictions sur le private test\n",
    "## based on  http://research.microsoft.com/pubs/258194/icmi2015_ChaZhang.pdf\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_microsoft.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_microsoft_iter_4000.caffemodel\"\n",
    "outfile_microsoft = TP_root+\"predictions/predictions_microsoft\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoft+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoft+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_microsoft,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.459714285714\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_microsoft et génère ses prédictions sur le private test\n",
    "MODEL_FILE_LENET = TP_root+\"model/lenet_microsoft.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_microsoftlr2_iter_10000.caffemodel\"\n",
    "outfile_microsoftlr2 = TP_root+\"predictions/predictions_microsoftlr2\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoftlr2+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoftlr2+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_microsoftlr2,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.173142857143\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET et génère ses prédictions sur le public test\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_train_test.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_train_test_iter_10000.caffemodel\"\n",
    "outfile_lenet = TP_root+\"predictions/public_predictions_lenet\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_lenet+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_lenet+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_lenet,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.178857142857\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_monoconv et génère ses prédictions sur le public test\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_monoconv.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_monoconv_iter_10000.caffemodel\"\n",
    "outfile_monoconv = TP_root+\"predictions/public_predictions_monoconv\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_monoconv+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_monoconv+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_monoconv,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.180285714286\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_tripleconv et génère ses prédictions sur le public test\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_tripleconv.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_tripleconv_iter_10000.caffemodel\"\n",
    "outfile_tripleconv = TP_root+\"predictions/public_predictions_tripleconv\"\n",
    "derniere_couche=\"ip1\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_tripleconv+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_tripleconv+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_tripleconv,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.182285714286\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_3conv2ip et génère ses prédictions sur le public test\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_3conv2ip.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_3conv2ip_iter_10000.caffemodel\"\n",
    "outfile_3conv2ip = TP_root+\"predictions/public_predictions_3conv2ip\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_3conv2ip+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_3conv2ip+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_3conv2ip,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.178571428571\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_microsoft et génère ses prédictions sur le private test\n",
    "## based on  http://research.microsoft.com/pubs/258194/icmi2015_ChaZhang.pdf\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_microsoft.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_microsoft_iter_4000.caffemodel\"\n",
    "outfile_microsoft = TP_root+\"predictions/public_predictions_microsoft\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoft+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoft+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_microsoft,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.164571428571\n"
     ]
    }
   ],
   "source": [
    "##RUN LENET_microsoftlr2 et génère ses prédictions sur le public test\n",
    "MODEL_FILE_LENET = TP_root+\"model/public_lenet_microsoft.prototxt\"\n",
    "PRETRAINED_LENET = TP_root+\"pretrained/lenet_microsoftlr2_iter_10000.caffemodel\"\n",
    "outfile_microsoftlr2 = TP_root+\"predictions/public_predictions_microsoftlr2\"\n",
    "derniere_couche=\"ip2\"\n",
    "\n",
    "net = caffe.Net(MODEL_FILE_LENET,PRETRAINED_LENET, caffe.TEST)\n",
    "\n",
    "#Premier forward pour initier\n",
    "out = net.forward()\n",
    "pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoftlr2+\"_batch_0\",\"wb\"))\n",
    "taille_batch = net.blobs['data'].data.shape[0]\n",
    "N_Batch = nb_donnees / taille_batch\n",
    "Predictions = np.argmax(net.blobs[derniere_couche].data,1).reshape((1,taille_batch))\n",
    "Labels = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "\n",
    "#Suite des forward\n",
    "for i in np.arange(N_Batch-1):\n",
    "    out = net.forward()\n",
    "    pickle.dump(net.blobs[derniere_couche].data, open(outfile_microsoftlr2+\"_batch_\"+str(i+1),\"wb\"))\n",
    "    Predictions_tmp = np.argmax(net.blobs[derniere_couche].data,1).reshape((1, taille_batch))\n",
    "    Labels_tmp = np.copy(net.blobs['label'].data[:]).reshape((1, taille_batch))\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))\n",
    "    Labels = np.hstack((Labels, Labels_tmp))\n",
    "\n",
    "#Sauvegarde les predictions et les labels\n",
    "pickle.dump(Predictions, open(outfile_microsoftlr2,\"wb\"))\n",
    "pickle.dump(Labels,open(outfile_public_labels,\"wb\"))\n",
    "\n",
    "#Calcul de la précision\n",
    "indices = (Labels==Predictions)\n",
    "H= Labels[Labels==Predictions]\n",
    "tr = (H.size +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "\n",
    "#nettoie les variables\n",
    "del net\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n",
      "(3500,)\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "##PREDICTIONS où le vote de chaque NN est pondéré par sa précision sur le test\n",
    "##Résultat sur le Private test\n",
    "N_Batch = 35\n",
    "taille_batch= 100\n",
    "outfile_lenet=TP_root+\"predictions/predictions_lenet\"\n",
    "outfile_monoconv=TP_root+\"predictions/predictions_monoconv\"\n",
    "outfile_tripleconv=TP_root+\"predictions/predictions_tripleconv\"\n",
    "outfile_3conv2ip=TP_root+\"predictions/predictions_3conv2ip\"\n",
    "outfile_microsoft=TP_root+\"predictions/predictions_microsoft\"\n",
    "outfile_microsoftlr2=TP_root+\"predictions/predictions_microsoftlr2\"\n",
    "\n",
    "outfile_labels=TP_root+\"labels\"\n",
    "Labels=pickle.load(open(outfile_labels,\"r\"))\n",
    "\n",
    "\n",
    "Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_0\",\"r\"))\n",
    "Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_0\",\"r\"))\n",
    "Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_0\",\"r\"))\n",
    "Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_0\",\"r\"))\n",
    "Predictions_global = Predictions_lenet*0.5207 \n",
    "Predictions_global += Predictions_monoconv*0.4797 \n",
    "Predictions_global += Predictions_tripleconv*0.4702\n",
    "Predictions_global += Predictions_3conv2ip * 0.4549 \n",
    "Predictions_global += Predictions_microsoft * 0.468\n",
    "Predictions_global += Predictions_microsoftlr2 * 0.4597\n",
    "\n",
    "Predictions= np.argmax(Predictions_global,1).reshape((1, taille_batch))\n",
    "\n",
    "for i in np.arange(N_Batch-1):\n",
    "    Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_global = Predictions_lenet*0.5207 \n",
    "    Predictions_global += Predictions_monoconv*0.4797\n",
    "    Predictions_global += Predictions_tripleconv*0.4702 \n",
    "    Predictions_global += Predictions_3conv2ip * 0.4549 \n",
    "    Predictions_global += Predictions_microsoft * 0.468\n",
    "    Predictions_global += Predictions_microsoftlr2 * 0.4597\n",
    "\n",
    "    Predictions_tmp = np.argmax(Predictions_global,1).reshape((1, taille_batch))\n",
    "\n",
    "    Predictions = np.hstack((Predictions, Predictions_tmp))       \n",
    "\n",
    "print Predictions[0].shape\n",
    "print Labels[0].shape\n",
    "inc=0\n",
    "for i in range(Labels.size):\n",
    "    if (int(Labels[0][i]) == Predictions[0][i]):\n",
    "        inc = inc +1\n",
    "\n",
    "tr = (inc +0.0) / (Labels.size +0.0)\n",
    "print tr\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 5.]\n",
      " [ 6.]]\n",
      "0.999142857143\n"
     ]
    }
   ],
   "source": [
    "##train un SVM sur les predictions précédentes sur le private test\n",
    "N_Batch = 35\n",
    "taille_batch= 100\n",
    "outfile_lenet=TP_root+\"predictions/predictions_lenet\"\n",
    "outfile_monoconv=TP_root+\"predictions/predictions_monoconv\"\n",
    "outfile_tripleconv=TP_root+\"predictions/predictions_tripleconv\"\n",
    "outfile_3conv2ip=TP_root+\"predictions/predictions_3conv2ip\"\n",
    "outfile_microsoft=TP_root+\"predictions/predictions_microsoft\"\n",
    "outfile_microsoftlr2=TP_root+\"predictions/predictions_microsoftlr2\"\n",
    "\n",
    "outfile_labels=TP_root+\"labels\"\n",
    "Labels=pickle.load(open(outfile_labels,\"r\"))\n",
    "Labels_vertical=np.zeros((3500,1))\n",
    "for i in np.arange(3500):\n",
    "    Labels_vertical[i,0]=Labels[0,i]\n",
    "print Labels_vertical\n",
    "\n",
    "Predictions_table=np.zeros((3500,6*7-1))\n",
    "\n",
    "##Créer un tableau de prédictions [3500 * 41]. Ligne = entrées. Colonne = features (prédictions des NN mis bout à bout).\n",
    "#Premier run initial\n",
    "Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_0\",\"r\"))\n",
    "Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_0\",\"r\"))\n",
    "Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_0\",\"r\"))\n",
    "Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_0\",\"r\"))\n",
    "for i in np.arange(100):\n",
    "    Predictions_table[i,0:6]=Predictions_lenet[i,0:6]\n",
    "    Predictions_table[i,7:13]=Predictions_monoconv[i,0:6]\n",
    "    Predictions_table[i,14:20]=Predictions_tripleconv[i,0:6]\n",
    "    Predictions_table[i,21:27]=Predictions_3conv2ip[i,0:6]\n",
    "    Predictions_table[i,28:34]=Predictions_microsoft[i,0:6]\n",
    "    Predictions_table[i,35:41]=Predictions_microsoftlr2[i,0:6]\n",
    "\n",
    "for i in np.arange(N_Batch-1):\n",
    "    Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_\"+str(i+1),\"r\"))\n",
    "    for j in np.arange(100):\n",
    "        Predictions_table[(i+1)*100+j,0:6]=Predictions_lenet[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,7:13]=Predictions_monoconv[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,14:20]=Predictions_tripleconv[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,21:27]=Predictions_3conv2ip[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,28:34]=Predictions_microsoft[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,35:41]=Predictions_microsoftlr2[j,0:6]   \n",
    "\n",
    "#SVM\n",
    "clf=svm.SVC(decision_function_shape='ovr')\n",
    "clf.fit(Predictions_table,Labels_vertical)\n",
    "\n",
    "Predictions=np.zeros((3500,1))\n",
    "#for i in np.arange(3500):\n",
    "Predictions=clf.predict(Predictions_table)\n",
    "\n",
    "inc=0\n",
    "for i in range(Labels_vertical.size):\n",
    "    if (int(Labels_vertical[i]) == Predictions[i]):\n",
    "        inc = inc +1\n",
    "tr = (inc +0.0) / (Labels.size +0.0)\n",
    "\n",
    "print tr\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 2.]]\n",
      "0.170857142857\n"
     ]
    }
   ],
   "source": [
    "##train un SVM sur les predictions précédentes sur le public test\n",
    "N_Batch = 35\n",
    "taille_batch= 100\n",
    "outfile_lenet=TP_root+\"predictions/public_predictions_lenet\"\n",
    "outfile_monoconv=TP_root+\"predictions/public_predictions_monoconv\"\n",
    "outfile_tripleconv=TP_root+\"predictions/public_predictions_tripleconv\"\n",
    "outfile_3conv2ip=TP_root+\"predictions/public_predictions_3conv2ip\"\n",
    "outfile_microsoft=TP_root+\"predictions/public_predictions_microsoft\"\n",
    "outfile_microsoftlr2=TP_root+\"predictions/public_predictions_microsoftlr2\"\n",
    "\n",
    "outfile_labels=TP_root+\"public_labels\"\n",
    "Labels=pickle.load(open(outfile_labels,\"r\"))\n",
    "Labels_vertical=np.zeros((3500,1))\n",
    "for i in np.arange(3500):\n",
    "    Labels_vertical[i,0]=Labels[0,i]\n",
    "print Labels_vertical\n",
    "\n",
    "Predictions_table=np.zeros((3500,6*7-1))\n",
    "\n",
    "##Créer un tableau de prédictions [3500 * 41]. Ligne = entrées. Colonne = features (prédictions des NN mis bout à bout).\n",
    "#Premier run initial\n",
    "Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_0\",\"r\"))\n",
    "Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_0\",\"r\"))\n",
    "Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_0\",\"r\"))\n",
    "Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_0\",\"r\"))\n",
    "Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_0\",\"r\"))\n",
    "for i in np.arange(100):\n",
    "    Predictions_table[i,0:6]=Predictions_lenet[i,0:6]\n",
    "    Predictions_table[i,7:13]=Predictions_monoconv[i,0:6]\n",
    "    Predictions_table[i,14:20]=Predictions_tripleconv[i,0:6]\n",
    "    Predictions_table[i,21:27]=Predictions_3conv2ip[i,0:6]\n",
    "    Predictions_table[i,28:34]=Predictions_microsoft[i,0:6]\n",
    "    Predictions_table[i,35:41]=Predictions_microsoftlr2[i,0:6]\n",
    "\n",
    "for i in np.arange(N_Batch-1):\n",
    "    Predictions_lenet = pickle.load(open(outfile_lenet+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_monoconv = pickle.load(open(outfile_monoconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_tripleconv = pickle.load(open(outfile_tripleconv+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_3conv2ip= pickle.load(open(outfile_3conv2ip+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoft= pickle.load(open(outfile_microsoft+\"_batch_\"+str(i+1),\"r\"))\n",
    "    Predictions_microsoftlr2= pickle.load(open(outfile_microsoftlr2+\"_batch_\"+str(i+1),\"r\"))\n",
    "    for j in np.arange(100):\n",
    "        Predictions_table[(i+1)*100+j,0:6]=Predictions_lenet[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,7:13]=Predictions_monoconv[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,14:20]=Predictions_tripleconv[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,21:27]=Predictions_3conv2ip[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,28:34]=Predictions_microsoft[j,0:6]\n",
    "        Predictions_table[(i+1)*100+j,35:41]=Predictions_microsoftlr2[j,0:6]   \n",
    "\n",
    "Predictions=np.zeros((3500,1))\n",
    "#for i in np.arange(3500):\n",
    "Predictions=clf.predict(Predictions_table)\n",
    "\n",
    "inc=0\n",
    "for i in range(Labels_vertical.size):\n",
    "    if (int(Labels_vertical[i]) == Predictions[i]):\n",
    "        inc = inc +1\n",
    "tr = (inc +0.0) / (Labels.size +0.0)\n",
    "\n",
    "print tr\n",
    "del Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": "Use the pre-trained ImageNet model to classify images with the Python interface.",
  "example_name": "ImageNet classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
